# config/constants.py
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class Paths:
    """パス定数管理"""
    PROJECT_ROOT: Path = Path(r"C:\Projects_workspace\03_unified_system")
    RAW_DATA: Path = PROJECT_ROOT / "data" / "raw"
    SQLITE_DB: Path = PROJECT_ROOT / "data" / "sqlite" / "main.db"
    ACCESS_OUTPUT: Path = PROJECT_ROOT / "data" / "access"
    LOGS: Path = PROJECT_ROOT / "logs"
    
    def __post_init__(self):
        """ディレクトリ作成"""
        for path in [self.RAW_DATA, self.SQLITE_DB.parent, self.ACCESS_OUTPUT, self.LOGS]:
            path.mkdir(parents=True, exist_ok=True)

@dataclass 
class FilePatterns:
    """ファイルパターン定義"""
    SAP_FILES: Dict[str, str] = None
    
    def __post_init__(self):
        self.SAP_FILES = {
            "PLM": "GetPLMItmPlntInfo_*.txt",
            "WBS": "GetSekkeiWBSJisseki.txt", 
            "BOM": "MARA_DL.csv",
            "SEISAN": "PP_DL_CSV_SEISAN_YOTEI.csv",
            "ZAIKO": "dbo_提出用_経理_滞留在庫資料_通常.xlsx"
        }

@dataclass
class ProcessConfig:
    """処理設定"""
    CHUNK_SIZE: int = 50000
    MAX_WORKERS: int = 4
    TIMEOUT_SECONDS: int = 300
    ENCODING_LIST: List[str] = None
    
    def __post_init__(self):
        self.ENCODING_LIST = ['utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']

# =============================================================================
# src/core/sqlite_manager.py
import sqlite3
import pandas as pd
import logging
from pathlib import Path
from typing import Optional, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import time

class SQLiteManager:
    """SQLite統一管理クラス"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.logger = self._setup_logger()
        self._init_database()
    
    def _setup_logger(self) -> logging.Logger:
        """ログ設定"""
        logger = logging.getLogger(self.__class__.__name__)
        if not logger.handlers:
            handler = logging.FileHandler(
                Paths().LOGS / f"{self.__class__.__name__}.log",
                encoding='utf-8'
            )
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _init_database(self):
        """データベース初期化"""
        with sqlite3.connect(self.db_path) as conn:
            # 性能向上設定
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL") 
            conn.execute("PRAGMA cache_size = 10000")
            conn.execute("PRAGMA temp_store = memory")
    
    def bulk_insert_from_csv(self, file_path: Path, table_name: str, 
                           chunk_size: int = 50000, encoding: str = 'utf-8') -> bool:
        """CSV大容量一括挿入"""
        start_time = time.time()
        self.logger.info(f"開始: {table_name} <- {file_path.name}")
        
        try:
            # エンコーディング自動判定
            if encoding == 'auto':
                encoding = self._detect_encoding(file_path)
            
            with sqlite3.connect(self.db_path) as conn:
                # テーブル削除・作成
                conn.execute(f"DROP TABLE IF EXISTS {table_name}")
                
                total_rows = 0
                for i, chunk in enumerate(pd.read_csv(file_path, 
                                                   chunksize=chunk_size,
                                                   encoding=encoding,
                                                   low_memory=False)):
                    
                    # 初回はテーブル作成
                    if_exists = 'replace' if i == 0 else 'append'
                    
                    # データ型最適化
                    chunk = self._optimize_dtypes(chunk)
                    
                    chunk.to_sql(table_name, conn, 
                               if_exists=if_exists, 
                               index=False, 
                               method='multi')
                    
                    total_rows += len(chunk)
                    self.logger.info(f"  処理中: {total_rows:,} rows")
                
                # インデックス作成
                self._create_indexes(conn, table_name, chunk.columns)
                
            elapsed = time.time() - start_time
            self.logger.info(f"完了: {table_name} ({total_rows:,} rows, {elapsed:.1f}s)")
            return True
            
        except Exception as e:
            self.logger.error(f"エラー: {table_name} - {str(e)}")
            return False
    
    def _detect_encoding(self, file_path: Path) -> str:
        """エンコーディング自動判定"""
        import chardet
        
        with open(file_path, 'rb') as f:
            raw_data = f.read(100000)  # 100KB読み取り
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result['confidence']
            
        self.logger.info(f"エンコーディング判定: {encoding} (信頼度: {confidence:.2f})")
        return encoding if confidence > 0.7 else 'shift_jis'
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """データ型最適化"""
        for col in df.columns:
            if df[col].dtype == 'object':
                # 数値変換試行
                try:
                    # まず整数変換
                    if df[col].str.match(r'^-?\d+$').all():
                        df[col] = pd.to_numeric(df[col], downcast='integer')
                    # 次に浮動小数点
                    elif df[col].str.match(r'^-?\d*\.?\d+$').all():
                        df[col] = pd.to_numeric(df[col], downcast='float')
                except:
                    pass
            elif df[col].dtype in ['int64', 'float64']:
                df[col] = pd.to_numeric(df[col], downcast='integer' if 'int' in str(df[col].dtype) else 'float')
        
        return df
    
    def _create_indexes(self, conn: sqlite3.Connection, table_name: str, columns: List[str]):
        """インデックス作成"""
        # 主要カラムの推定とインデックス作成
        key_patterns = ['id', 'code', 'key', 'no', '番号', 'コード']
        
        for col in columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in key_patterns):
                try:
                    idx_name = f"idx_{table_name}_{col.replace(' ', '_')}"
                    conn.execute(f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table_name} ({col})")
                    self.logger.info(f"  インデックス作成: {idx_name}")
                except Exception as e:
                    self.logger.warning(f"  インデックス作成失敗: {col} - {str(e)}")
    
    def export_to_access_format(self, table_name: str, output_path: Path) -> bool:
        """Access互換形式でエクスポート"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
                
                # Access互換データ型変換
                for col in df.columns:
                    if df[col].dtype == 'object':
                        # 文字列は255文字制限
                        df[col] = df[col].astype(str).str[:255]
                    elif 'datetime' in str(df[col].dtype):
                        # 日付形式統一
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                
                # CSV出力（Access取り込み用）
                df.to_csv(output_path, index=False, encoding='shift_jis')
                self.logger.info(f"Access用エクスポート完了: {output_path}")
                return True
                
        except Exception as e:
            self.logger.error(f"エクスポートエラー: {str(e)}")
            return False
    
    def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """テーブル情報取得"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # 行数
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = cursor.fetchone()[0]
            
            # カラム情報
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            
            # サイズ情報
            cursor.execute(f"SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            db_size = cursor.fetchone()[0]
            
            return {
                'row_count': row_count,
                'columns': columns,
                'db_size_mb': db_size / 1024 / 1024
            }

# =============================================================================
# src/core/file_processor.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import glob
from typing import List, Tuple

class FileProcessor:
    """ファイル処理統合クラス"""
    
    def __init__(self):
        self.paths = Paths()
        self.config = ProcessConfig()
        self.sqlite_mgr = SQLiteManager(self.paths.SQLITE_DB)
        self.logger = self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger(self.__class__.__name__)
        if not logger.handlers:
            handler = logging.FileHandler(
                self.paths.LOGS / f"{self.__class__.__name__}.log",
                encoding='utf-8'
            )
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def process_all_files(self) -> Dict[str, bool]:
        """全ファイル一括処理"""
        start_time = time.time()
        self.logger.info("=== ファイル一括処理開始 ===")
        
        # 処理対象ファイル取得
        file_tasks = self._get_file_tasks()
        results = {}
        
        # 並行処理実行
        with ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS) as executor:
            future_to_task = {
                executor.submit(self._process_single_file, task): task 
                for task in file_tasks
            }
            
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result(timeout=self.config.TIMEOUT_SECONDS)
                    results[task['table_name']] = result
                except Exception as e:
                    self.logger.error(f"ファイル処理エラー: {task['file_path']} - {str(e)}")
                    results[task['table_name']] = False
        
        # 結果集計
        success_count = sum(results.values())
        total_count = len(results)
        elapsed = time.time() - start_time
        
        self.logger.info(f"=== 処理完了: {success_count}/{total_count} 成功 ({elapsed:.1f}s) ===")
        return results
    
    def _get_file_tasks(self) -> List[Dict[str, Any]:
        """処理対象ファイル一覧作成"""
        tasks = []
        patterns = FilePatterns().SAP_FILES
        
        for key, pattern in patterns.items():
            file_paths = list(self.paths.RAW_DATA.glob(pattern))
            for file_path in file_paths:
                if file_path.is_file():
                    tasks.append({
                        'file_path': file_path,
                        'table_name': f"{key}_{file_path.stem}".lower(),
                        'type': key
                    })
        
        self.logger.info(f"処理対象ファイル: {len(tasks)} 件")
        return tasks
    
    def _process_single_file(self, task: Dict[str, Any]) -> bool:
        """単一ファイル処理"""
        file_path = task['file_path']
        table_name = task['table_name']
        
        # ファイル種別別処理
        if file_path.suffix.lower() == '.csv':
            return self.sqlite_mgr.bulk_insert_from_csv(
                file_path, table_name, 
                chunk_size=self.config.CHUNK_SIZE,
                encoding='auto'
            )
        elif file_path.suffix.lower() in ['.txt']:
            # TXTファイルはCSVとして処理（区切り文字自動判定）
            return self._process_txt_file(file_path, table_name)
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            return self._process_excel_file(file_path, table_name)
        else:
            self.logger.warning(f"未対応ファイル形式: {file_path}")
            return False
    
    def _process_txt_file(self, file_path: Path, table_name: str) -> bool:
        """TXTファイル処理（区切り文字自動判定）"""
        try:
            # 区切り文字判定
            with open(file_path, 'r', encoding='shift_jis') as f:
                first_line = f.readline()
                
            # 区切り文字候補
            separators = ['\t', ',', '|', ' ']
            best_sep = '\t'  # デフォルト
            max_cols = 0
            
            for sep in separators:
                cols = len(first_line.split(sep))
                if cols > max_cols:
                    max_cols = cols
                    best_sep = sep
            
            # pandasで読み込み、SQLiteに保存
            df = pd.read_csv(file_path, sep=best_sep, encoding='shift_jis')
            
            with sqlite3.connect(self.sqlite_mgr.db_path) as conn:
                df.to_sql(table_name, conn, if_exists='replace', index=False)
            
            self.logger.info(f"TXT処理完了: {table_name} ({len(df)} rows)")
            return True
            
        except Exception as e:
            self.logger.error(f"TXT処理エラー: {file_path} - {str(e)}")
            return False
    
    def _process_excel_file(self, file_path: Path, table_name: str) -> bool:
        """Excelファイル処理"""
        try:
            # 全シート読み込み
            excel_file = pd.ExcelFile(file_path)
            
            with sqlite3.connect(self.sqlite_mgr.db_path) as conn:
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    sheet_table_name = f"{table_name}_{sheet_name}".lower()
                    df.to_sql(sheet_table_name, conn, if_exists='replace', index=False)
                    self.logger.info(f"Excel処理: {sheet_table_name} ({len(df)} rows)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Excel処理エラー: {file_path} - {str(e)}")
            return False

# =============================================================================
# メイン実行スクリプト
if __name__ == "__main__":
    # 設定初期化
    paths = Paths()
    
    # ファイル処理実行
    processor = FileProcessor()
    results = processor.process_all_files()
    
    # 結果表示
    print("\n=== 処理結果 ===")
    for table, success in results.items():
        status = "✅ 成功" if success else "❌ 失敗"
        print(f"{table}: {status}")
    
    print(f"\n成功率: {sum(results.values())}/{len(results)} ({sum(results.values())/len(results)*100:.1f}%)")
